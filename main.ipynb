{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "61a5a5ba5b80c5585c4ec4fbf5ff78995a71b34c1febecade75eb8e21174053c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project C116"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pandas\r\n",
    "\r\n",
    "data_frame = pandas.read_csv(\"https://raw.githubusercontent.com/whitehatjr/datasets/master/pro-c116/Admission_Predict.csv\")\r\n",
    "\r\n",
    "toefl_score = data_frame[\"TOEFL Score\"].to_list()\r\n",
    "gre_score = data_frame[\"GRE Score\"].to_list()\r\n",
    "coa = data_frame[\"Chance of admit\"].to_list()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Showing Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import plotly.express as px\r\n",
    "\r\n",
    "figure = px.scatter(x=toefl_score, y=gre_score, color=coa, labels=dict(x=\"TOEFL Score\", y=\"GRE Score\", color=\"Chance of Admitting\"), title=\"TOEFL Score vs GRE Score\")\r\n",
    "\r\n",
    "figure.update_traces(marker=dict(line=dict(color='DarkSlateGrey')))\r\n",
    "\r\n",
    "figure.show()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MAHESH~1\\AppData\\Local\\Temp/ipykernel_2616/1485552625.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_traces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'DarkSlateGrey'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\plotly\\basedatatypes.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3396\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3398\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3400\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\plotly\\io\\_renderers.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnbformat\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbformat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"4.2.0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    398\u001b[0m                 \u001b[1;34m\"Mime type rendering requires nbformat>=4.2.0 but it is not installed\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Test Split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "scores = data_frame[[\"TOEFL Score\", \"GRE Score\"]]\r\n",
    "\r\n",
    "scores_train, scores_test, coa_train, coa_test = train_test_split(scores, coa, test_size=0.25, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\r\n",
    "\r\n",
    "log_reg = LogisticRegression(random_state=0)\r\n",
    "log_reg.fit(scores_train, coa_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction Accuracy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "prediction = log_reg.predict(scores_test)\r\n",
    "accuracy = accuracy_score(coa_test, prediction)\r\n",
    "\r\n",
    "print(f\"Accuracy of prediction: {accuracy}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of prediction: 0.77\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "ss = StandardScaler()\r\n",
    "ss.fit_transform(scores_train)\r\n",
    "\r\n",
    "user_gre = int(input(\"What is your GRE score: \"))\r\n",
    "user_toefl = int(input(\"What is your TOEFL score: \"))\r\n",
    "\r\n",
    "user_test = ss.transform([[user_gre, user_toefl]])\r\n",
    "user_prediction = log_reg.predict(user_test)\r\n",
    "\r\n",
    "if user_prediction[0] == 1:\r\n",
    "    print(\"User May Pass\")\r\n",
    "else:\r\n",
    "    print(\"You get to go to Elon Musk's House\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}